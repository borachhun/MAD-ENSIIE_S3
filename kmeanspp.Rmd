---
title: "Variant of k-means for acceleration and better convergence"
author: "Piseth KHENG, Borachhun YOU"
date: "24 October 2022"
output: pdf_document
classoption: a4paper
---

# Exercise 1: `k-means++` algorithm

### 1. Programming `k-means++` algorithm

`k-means++` is an algorithm proposed by David Arthur and Sergei Vassilvitskii with the goal of improving the convergence and speed of the `k-means` algorithm, and it does so by carefully choosing the center of the clusters at the initial step. With `k-means`, it initially chooses the centers uniformly at random from the data points. In contrast, `k-means++` chooses the centers as followed:

i. Choose one center $c_1$ uniformly at random from the data points $\mathcal{X}$
ii. Choose the next center $c_i$ by selecting $x' \in \mathcal{X}$ with weighted probability $\frac{D(x')^2}{\sum_{x \in \mathcal{X}} D(x)^2}$
iii. Repeat step ii until a total of $k$ centers are chosen, where $k$ is a given number of clusters.

After choosing the centers, `k-means++` proceeds the same calculations as `k-means`.

```{r}
kmeanspp <- function(X, k) {
  X <- as.matrix(X)
  X_row <- nrow(X)
  
  # choose first center uniformly at random
  center_index <- sample(1:X_row, size=1)
  
  for (i in 2:k) {
    
    # calculate squared distance to closest chosen center
    D2 <- c()
    for (d in 1:X_row) {
      data_centers_dist_sq <- c()
      for (c in center_index) {
        data_centers_dist_sq <- c(data_centers_dist_sq, sum((X[d,]-X[c,])^2))
      }
      D2 <- c(D2, min(data_centers_dist_sq))
    }
    
    # choose a new center
    center_index <- c(center_index, sample(1:X_row, size=1, prob=(D2/sum(D2))))
  }
  
  # run kmeans with initialized centers
  return(kmeans(X, centers=X[center_index,]))
}
```

### 2. Simulate `NORM-10` and `NORM-25` datasets

We now simulate 2 datasets, `NORM-10` and `NORM-25`, for evaluating the performance of the `k-means++` algorithm. To generate the datasets, we choose 10 (or 25) "real" centers uniformly at random from a hypercube of side length 500. We then add points from Gaussian distributions of variance 1 around each real center.

- For `NORM-10`, we generate 1000 data around each of the 10 centers of dimension 5
- For `NORM-25`, we generate 400 data around each of the 25 centers of dimension 15.

```{r}
# n = number of centers
# dim = dimension of data
# pts = number of points around each center
NORM <- function(n, dim, pts) {
  set.seed(NULL)
  
  # choose true centers
  center_coor <- c()
  for (i in 1:(n*dim)) {
    center_coor <- c(center_coor, runif(1, min=0, max=500))
  }
  true_centers <- matrix(center_coor, nrow=n, ncol=dim, byrow=TRUE)
  
  # add points around centers
  res <- c()
  for (ct_row in 1:n) {
    for (i in 1:pts) {
      for (ct_col in 1:dim) {
        res <- c(res, rnorm(1, mean=true_centers[ct_row, ct_col], sd=1))
      }
    }
  }
  
  return(matrix(res, ncol=dim, byrow=TRUE))
}

`NORM-10` <- NORM(n=10, dim=5, pts=1000)
`NORM-25` <- NORM(n=25, dim=15, pts=400)
```

### 3. `k-means` and `k-means++` comparison

For the `k-means` problem (thus `k-means++` as well), we wish to minimize

$$\phi = \sum_{x \in \mathcal{X}} \min_{c \in \mathcal{C}} ||x-c||^2$$
where $\mathcal{X}$ is the set of data points and $\mathcal{C}$ is the set of centers.

We now use the value of $\phi$ and the execution time of both algorithms to compare the perfomance between the two.

```{r}
phi <- function(X, centers) {
  res <- 0
  for (x_row in 1:nrow(X)) {
    to_min <- c()
    for (c_row in 1:nrow(centers)) {
      to_min <- c(to_min, sum((X[x_row,] - centers[c_row,])^2))
    }
    res <- res + min(to_min)
  }
  return(res)
}
```

```{r}
perf <- function(dataset) {
  T_km <- c()
  T_kmpp <- c()
  
  phi_km <- c()
  phi_kmpp <- c()
  
  for (k in c(10,25,50)) {
    for (i in 1:20) {
      Sys.time() -> begin_km
      km <- kmeans(dataset, k)
      Sys.time() -> end_km
      
      Sys.time() -> begin_kmpp
      kmpp <- kmeanspp(dataset, k)
      Sys.time() -> end_kmpp
      
      T_km <- c(T_km, end_km-begin_km)
      T_kmpp <- c(T_kmpp, end_kmpp-begin_kmpp)
      phi_km <- c(phi_km, phi(dataset, km$centers))
      phi_kmpp <- c(phi_kmpp, phi(dataset, kmpp$centers))
    }
  }
  
  return(list(
    T_km_10 = T_km[1:20],
    T_km_25 = T_km[21:40],
    T_km_50 = T_km[41:60],
    
    T_kmpp_10 = T_kmpp[1:20],
    T_kmpp_25 = T_kmpp[21:40],
    T_kmpp_50 = T_kmpp[41:60],
    
    phi_km_10 = phi_km[1:20],
    phi_km_25 = phi_km[21:40],
    phi_km_50 = phi_km[41:60],
    
    phi_kmpp_10 = phi_kmpp[1:20],
    phi_kmpp_25 = phi_kmpp[21:40],
    phi_kmpp_50 = phi_kmpp[41:60]
  ))
}
```

```{r}
perf_NORM_10 <- perf(`NORM-10`)
perf_NORM_25 <- perf(`NORM-25`)
```


\begin{table}
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
 & \multicolumn{2}{|c|}{Average $\phi$} & \multicolumn{2}{|c|}{Minimum $\phi$} & \multicolumn{2}{|c|}{Average $T$} \\
k & \multicolumn{1}{|c}{\texttt{k-means}} & \multicolumn{1}{c|}{\texttt{k-means++}} & \multicolumn{1}{|c}{\texttt{k-means}} & \multicolumn{1}{c|}{\texttt{k-means++}} & \multicolumn{1}{|c}{\texttt{k-means}} & \multicolumn{1}{c|}{\texttt{k-means++}} \\
\hline
10 & `r mean(perf_NORM_10$phi_km_10)` & `r mean(perf_NORM_10$phi_kmpp_10)` & `r min(perf_NORM_10$phi_km_10)` & `r min(perf_NORM_10$phi_kmpp_10)` & `r mean(perf_NORM_10$T_km_10)` & `r mean(perf_NORM_10$T_kmpp_10)` \\
25 & `r mean(perf_NORM_10$phi_km_25)` & `r mean(perf_NORM_10$phi_kmpp_25)` & `r min(perf_NORM_10$phi_km_25)` & `r min(perf_NORM_10$phi_kmpp_25)` & `r mean(perf_NORM_10$T_km_25)` & `r mean(perf_NORM_10$T_kmpp_25)` \\
50 & `r mean(perf_NORM_10$phi_km_50)` & `r mean(perf_NORM_10$phi_kmpp_50)` & `r min(perf_NORM_10$phi_km_50)` & `r min(perf_NORM_10$phi_kmpp_50)` & `r mean(perf_NORM_10$T_km_50)` & `r mean(perf_NORM_10$T_kmpp_50)` \\
\hline
\end{tabular}
\caption{Experimental results on the \emph{Norm-10} dataset (n = 10000, d = 5)}
\end{table}

# Exercise 2: `iris` dataset

### 1. Apply `k-means++`, `k-means` and `Mclust` on `iris` dataset

```{r}
data_iris <- iris[,1:4]
real_class <- iris[,5]

```

```{r}
kmpp <- kmeanspp(data_iris, 3)
kmpp_class <- kmpp$cluster
#table(kmpp_class, real_class)
kmpp.cluster<-as.factor(kmpp$cluster) 
kmpp.centers <- as.data.frame(kmpp$centers)
```

```{r}
km <- kmeans(data_iris, centers=3)
km_class <- km$cluster
#table(km_class, real_class)
km.cluster<-as.factor(km$cluster) 
km.centers <- as.data.frame(km$centers)
```


```{r}
library(mclust)
m <- Mclust(data_iris, G=3)
m_class <- m$classification
m.center <- as.factor(m$classification)
#table(m_class, real_class)
#summary(m)
```

Compare the performance of `k-means++`, `k-means` and `Mclust` on `iris` dataset
```{r}
Sys.time() -> begin_km
perf_iris_km <- kmeans(data_iris, 3)
Sys.time() -> end_km
T_km <- end_km-begin_km

Sys.time() -> begin_km
perf_iris_kmpp <- kmeanspp(data_iris, 3)
Sys.time() -> end_km
T_kmpp <- end_km-begin_km

Sys.time() -> begin_km
perf_iris_gmm <- Mclust(data_iris, G=3)
Sys.time() -> end_km
T_gmm <- end_km-begin_km
```

```{r}
T_km
T_kmpp
T_gmm
```
As the result, we clearly see that the executed times of Kmeans++ and Kmeans is quite similar when it comes to smaller datasets. Moreover, the performance of Mclust algorithm is much slower than Kmeans and Kmeans++.

```{r}
#par(mfrow=c(4,1))
#pairs(data_iris, col=as.numeric(iris$Species))
#pairs(data_iris, col=km.cluster)
#pairs(data_iris, col=kmpp.cluster)
#plot(m, what = "classification")
```

### 2. Visualize the different partitions on PCA plan

```{r}
library("factoextra")
iris_scale <- scale(data_iris)
iris.pca <- PCA(iris_scale, ncp = 4, graph = FALSE)
```

```{r}
iris.plot <- fviz_pca_ind(iris.pca, label="none", habillage = iris$Species, title="Original classes")
iris.plot.kmeanspp <- fviz_pca_ind(iris.pca, label="none", habillage = kmpp.cluster, title = "Kmeans++")
iris.plot.kmeans <- fviz_pca_ind(iris.pca, label="none", habillage = km.cluster, title = "kmeans")
iris.plot.mclust <- fviz_pca_ind(iris.pca, label="none", habillage = m.center, title = "Mclust")
```
#### Comparing the partitions of original data with `Kmeans++`, `Kmeans` and `Mclust`
```{r}
iris.plot
iris.plot.kmeanspp
iris.plot.kmeans
iris.plot.mclust
```
### Comments.
After the visualization, we can see that the partition from Mclust algorithm quite similar to the partition of original dataset even thougth it takes so long to finish the process.
